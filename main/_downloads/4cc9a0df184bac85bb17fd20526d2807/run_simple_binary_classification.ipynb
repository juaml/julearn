{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Simple Binary Classification\n\nThis example uses the 'iris' dataset and performs a simple binary\nclassification using a Support Vector Machine classifier.\n\n.. include:: ../../links.inc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Federico Raimondo <f.raimondo@fz-juelich.de>\n#\n# License: AGPL\nfrom seaborn import load_dataset\nfrom julearn import run_cross_validation\nfrom julearn.utils import configure_logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the logging level to info to see extra information\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "configure_logging(level='INFO')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_iris = load_dataset('iris')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset has three kind of species. We will keep two to perform a binary\nclassification.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_iris = df_iris[df_iris['species'].isin(['versicolor', 'virginica'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As features, we will use the sepal length, width and petal length.\nWe will try to predict the species.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = ['sepal_length', 'sepal_width', 'petal_length']\ny = 'species'\nscores = run_cross_validation(\n    X=X, y=y, data=df_iris, model='svm', preprocess_X='zscore')\n\nprint(scores['test_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally, we can choose to assess the performance of the model using\ndifferent scoring functions.\n\nFor example, we might have an unbalanced dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_unbalanced = df_iris[20:]  # drop the first 20 versicolor samples\nprint(df_unbalanced['species'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we compute the `accuracy`, we might not account for this imbalance. A more\nsuitable metric is the `balanced_accuracy`. More information in scikit-learn:\n`Balanced Accuracy`_\n\nWe will also set the random seed so we always split the data in the same way.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = run_cross_validation(\n    X=X, y=y, data=df_unbalanced, model='svm', seed=42, preprocess_X='zscore',\n    scoring=['accuracy', 'balanced_accuracy'])\n\nprint(scores['test_accuracy'].mean())\nprint(scores['test_balanced_accuracy'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other kind of metrics allows us to evaluate how good our model is to detect\nspecific targets. Suppose we want to create a model that correctly identifies\nthe `versicolor` samples.\n\nNow we might want to evaluate the precision score, or the ratio of true\npositives (tp) over all positives (true and false positives). More\ninformation in scikit-learn: `Precision`_\n\nFor this metric to work, we need to define which are our `positive` values.\nIn this example, we are interested in detecting `versicolor`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "precision_scores = run_cross_validation(\n    X=X, y=y, data=df_unbalanced, model='svm', preprocess_X='zscore', seed=42,\n    scoring='precision', pos_labels='versicolor')\nprint(precision_scores['test_score'].mean())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}