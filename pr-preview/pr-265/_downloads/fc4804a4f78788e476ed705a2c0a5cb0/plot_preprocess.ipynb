{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Preprocessing with variance threshold, zscore and PCA\n\nThis example uses the ``make_regression`` function to create a simple dataset,\nperforms a simple regression after the preprocessing of the features\nincluding removal of low variance features, feature normalization for only\ntwo features using zscore and feature reduction using PCA.\nWe will check the features after each preprocessing step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Shammi More <s.more@fz-juelich.de>\n#          Leonard Sasse <l.sasse@fz-juelich.de>\n# License: AGPL\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_regression\n\nfrom julearn import run_cross_validation\nfrom julearn.inspect import preprocess\nfrom julearn.pipeline import PipelineCreator\nfrom julearn.utils import configure_logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the logging level to info to see extra information.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "configure_logging(level=\"INFO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dataset using ``sklearn`` ``make_regression``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\nX, y = [f\"Feature {x}\" for x in range(1, 5)], \"y\"\ndf[X], df[y] = make_regression(\n    n_samples=100, n_features=4, n_informative=3, noise=0.3, random_state=0\n)\n\n# We only want to zscore the first two features, so let's get their names.\nfirst_two = X[:2]\n\n# We can define a dictionary, in which the 'key' defines the names of our\n# different 'types' of 'X'. The 'value' determine, which features belong to\n# this type.\nX_types = {\"X_to_zscore\": first_two}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the summary statistics of the raw features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Summary Statistics of the raw features : \\n\", df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will preprocess all features using variance thresholding.\nWe will only zscore the first two features, and then perform PCA using all\nfeatures. We will zscore the target and then train a random forest model.\nSince we use the PipelineCreator object we have to explicitly declare which\n`X_types` each preprocessing step should be applied to. If we do not declare\nthe type in the ``add`` method using the ``apply_to`` keyword argument,\nthe step will default to ``\"continuous\"`` or to another type that can be\ndeclared in the constructor of the ``PipelineCreator``.\nTo transform the target we could set ``apply_to=\"target\"``, which is a special\ntype, that cannot be user-defined. Please note also that if a step is added\nto transform the target, you also have to explicitly add the model that is\nto be used in the regression to the ``PipelineCreator``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Define model parameters and preprocessing steps first\n# The hyperparameters for each step can be added as a keyword argument and\n# should be either one parameter or an iterable of multiple parameters for a\n# search.\n\n# Setting the threshold for variance to 0.15, number of PCA components to 2\n# and number of trees for random forest to 200.\n\n# By setting \"apply_to=*\", we can apply the preprocessing step to all features.\npipeline_creator = PipelineCreator(problem_type=\"regression\")\n\npipeline_creator.add(\"select_variance\", apply_to=\"*\", threshold=0.15)\npipeline_creator.add(\"zscore\", apply_to=\"X_to_zscore\")\npipeline_creator.add(\"pca\", apply_to=\"*\", n_components=2)\npipeline_creator.add(\"rf\", apply_to=\"*\", n_estimators=200)\n\n# Because we have already added the model to the pipeline creator, we can\n# simply drop in the ``pipeline_creator`` as a model. If we did not add a model\n# here, we could add the ``pipeline_creator`` using the keyword argument\n# ``preprocess`` and hand over a model separately.\n\nscores, model = run_cross_validation(\n    X=X,\n    y=y,\n    X_types=X_types,\n    data=df,\n    model=pipeline_creator,\n    scoring=[\"r2\", \"neg_mean_absolute_error\"],\n    return_estimator=\"final\",\n    seed=200,\n)\n\n# We can use the final estimator to inspect the transformed features at a\n# specific step of the pipeline. Since the PCA was the last step added to the\n# pipeline, we can simply get the model up to this step by indexing as follows:\n\nX_after_pca = model[:-1].transform(df[X])\n\nprint(\"X after PCA:\")\nprint(\"=\" * 79)\nprint(X_after_pca)\n\n# We can select pipelines up to earlier steps by indexing previous elements\n# in the final estimator. For example, to inspect features after the zscoring\n# step:\n\nX_after_zscore = model[:-2].transform(df[X])\nprint(\"X after zscore:\")\nprint(\"=\" * 79)\nprint(X_after_zscore)\n\n# However, to make this less confusing you can also simply use the high-level\n# function ``preprocess`` to explicitly refer to a pipeline step by name:\n\nX_after_pca = preprocess(model, X=X, data=df, until=\"pca\")\nX_after_zscore = preprocess(model, X=X, data=df, until=\"zscore\")\n\n# Let's plot scatter plots for raw features and the PCA components.\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nsns.scatterplot(x=X[0], y=X[1], data=df, ax=axes[0])\naxes[0].set_title(\"Raw features\")\nsns.scatterplot(x=\"pca__pca0\", y=\"pca__pca1\", data=X_after_pca, ax=axes[1])\naxes[1].set_title(\"PCA components\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the summary statistics of the zscored features. We see here\nthat the mean of all the features is zero and standard deviation is one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    \"Summary Statistics of the zscored features : \\n\",\n    X_after_zscore.describe(),\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}