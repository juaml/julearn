{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Simple Binary Classification\n\nThis example uses the ``iris`` dataset and performs a simple binary\nclassification using a Support Vector Machine classifier.\n\n.. include:: ../../links.inc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Federico Raimondo <f.raimondo@fz-juelich.de>\n#\n# License: AGPL\n\nfrom seaborn import load_dataset\nfrom julearn import run_cross_validation\nfrom julearn.utils import configure_logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the logging level to info to see extra information\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "configure_logging(level=\"INFO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_iris = load_dataset(\"iris\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset has three kind of species. We will keep two to perform a binary\nclassification.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_iris = df_iris[df_iris[\"species\"].isin([\"versicolor\", \"virginica\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As features, we will use the sepal length, width and petal length.\nWe will try to predict the species.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = [\"sepal_length\", \"sepal_width\", \"petal_length\"]\ny = \"species\"\nscores = run_cross_validation(\n    X=X,\n    y=y,\n    data=df_iris,\n    model=\"svm\",\n    problem_type=\"classification\",\n    preprocess=\"zscore\",\n)\n\nprint(scores[\"test_score\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Additionally, we can choose to assess the performance of the model using\ndifferent scoring functions.\n\nFor example, we might have an unbalanced dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_unbalanced = df_iris[20:]  # drop the first 20 versicolor samples\nprint(df_unbalanced[\"species\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we compute the `accuracy`, we might not account for this imbalance. A more\nsuitable metric is the `balanced_accuracy`. More information in\n``scikit-learn``: :func:`~sklearn.metrics.balanced_accuracy_score`.\n\nWe will also set the random seed so we always split the data in the same way.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = run_cross_validation(\n    X=X,\n    y=y,\n    data=df_unbalanced,\n    model=\"svm\",\n    seed=42,\n    preprocess=\"zscore\",\n    problem_type=\"classification\",\n    scoring=[\"accuracy\", \"balanced_accuracy\"],\n)\n\nprint(scores[\"test_accuracy\"].mean())\nprint(scores[\"test_balanced_accuracy\"].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other kind of metrics allows us to evaluate how good our model is to detect\nspecific targets. Suppose we want to create a model that correctly identifies\nthe `versicolor` samples.\n\nNow we might want to evaluate the precision score, or the ratio of true\npositives (tp) over all positives (true and false positives). More\ninformation in ``scikit-learn``: :func:`~sklearn.metrics.precision_score`.\n\nFor this metric to work, we need to define which are our `positive` values.\nIn this example, we are interested in detecting `versicolor`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "precision_scores = run_cross_validation(\n    X=X,\n    y=y,\n    data=df_unbalanced,\n    model=\"svm\",\n    preprocess=\"zscore\",\n    problem_type=\"classification\",\n    seed=42,\n    scoring=\"precision\",\n    pos_labels=\"versicolor\",\n)\nprint(precision_scores[\"test_score\"].mean())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}