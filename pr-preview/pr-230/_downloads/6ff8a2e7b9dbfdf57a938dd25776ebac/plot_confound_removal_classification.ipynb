{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Confound Removal (model comparison)\n\nThis example uses the ``iris`` dataset, performs simple binary classification\nwith and without confound removal using a Random Forest classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Shammi More <s.more@fz-juelich.de>\n#          Federico Raimondo <f.raimondo@fz-juelich.de>\n#          Leonard Sasse <l.sasse@fz-juelich.de>\n# License: AGPL\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom seaborn import load_dataset\n\nfrom julearn import run_cross_validation\nfrom julearn.model_selection import StratifiedBootstrap\nfrom julearn.pipeline import PipelineCreator\nfrom julearn.utils import configure_logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the logging level to info to see extra information.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "configure_logging(level=\"INFO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the iris data from seaborn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_iris = load_dataset(\"iris\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset has three kind of species. We will keep two to perform a binary\nclassification.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_iris = df_iris[df_iris[\"species\"].isin([\"versicolor\", \"virginica\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As features, we will use the sepal length, width and petal length and use\npetal width as confound.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = [\"sepal_length\", \"sepal_width\", \"petal_length\"]\ny = \"species\"\nconfounds = [\"petal_width\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Doing hypothesis testing in ML is not that simple. If we were to use\nclassical frequentist statistics, we have the problem that using cross\nvalidation, the samples are not independent and the population (train + test)\nis always the same.\n\nIf we want to compare two models, an alternative is to contrast, for each\nfold, the performance gap between the models. If we combine that approach\nwith bootstrapping, we can then compare the confidence intervals of the\ndifference. If the 95% CI is above 0 (or below), we can claim that the models\nare different with p < 0.05.\n\nLet's use a bootstrap CV. In the interest of time we do 20 iterations,\nchange the number of bootstrap iterations to at least 2000 for a valid test.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_bootstrap = 20\nn_elements = len(df_iris)\ncv = StratifiedBootstrap(n_splits=n_bootstrap, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we will train a model without performing confound removal on features.\nNote: confounds by default.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores_ncr = run_cross_validation(\n    X=X,\n    y=y,\n    data=df_iris,\n    model=\"rf\",\n    cv=cv,\n    problem_type=\"classification\",\n    preprocess=\"zscore\",\n    scoring=[\"accuracy\", \"roc_auc\"],\n    return_estimator=\"cv\",\n    seed=200,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we train a model after performing confound removal on the features.\nNote: we initialize the CV again to use the same folds as before.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv = StratifiedBootstrap(n_splits=n_bootstrap, test_size=0.3, random_state=42)\n\n# In order to tell ``run_cross_validation`` which columns are confounds,\n# and which columns are features, we have to define the X_types:\nX_types = {\"features\": X, \"confound\": confounds}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now define a pipeline creator and add a confound removal step.\nThe pipeline creator should apply all the steps, by default, to the\nfeatures type.\n\nThe first step will zscore both features and confounds.\n\nThe second step will remove the confounds (type \"confound\") from the\n\"features\".\n\nFinally, a random forest will be trained.\nGiven the default ``apply_to`` in the pipeline creator,\nthe random forest will only be trained using \"features\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "creator = PipelineCreator(problem_type=\"classification\", apply_to=\"features\")\ncreator.add(\"zscore\", apply_to=[\"features\", \"confound\"])\ncreator.add(\"confound_removal\", apply_to=\"features\", confounds=\"confound\")\ncreator.add(\"rf\")\n\nscores_cr = run_cross_validation(\n    X=X + confounds,\n    y=y,\n    data=df_iris,\n    model=creator,\n    cv=cv,\n    X_types=X_types,\n    scoring=[\"accuracy\", \"roc_auc\"],\n    return_estimator=\"cv\",\n    seed=200,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can compare the accuracies. We can combine the two outputs as\n``pandas.DataFrame``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores_ncr[\"confounds\"] = \"Not Removed\"\nscores_cr[\"confounds\"] = \"Removed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we convert the metrics to a column for easier seaborn plotting (convert\nto long format).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "index = [\"fold\", \"confounds\"]\nscorings = [\"test_accuracy\", \"test_roc_auc\"]\n\ndf_ncr_metrics = scores_ncr.set_index(index)[scorings].stack()\ndf_ncr_metrics.index.names = [\"fold\", \"confounds\", \"metric\"]\ndf_ncr_metrics.name = \"value\"\n\ndf_cr_metrics = scores_cr.set_index(index)[scorings].stack()\ndf_cr_metrics.index.names = [\"fold\", \"confounds\", \"metric\"]\ndf_cr_metrics.name = \"value\"\n\ndf_metrics = pd.concat((df_ncr_metrics, df_cr_metrics))\n\ndf_metrics = df_metrics.reset_index()\ndf_metrics.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally plot the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sns.catplot(\n    x=\"confounds\", y=\"value\", col=\"metric\", data=df_metrics, kind=\"swarm\"\n)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While this plot allows us to see the mean performance values and compare\nthem, these samples are paired. In order to see if there is a systematic\ndifference, we need to check the distribution of differences between the\nthe models.\n\nFirst, we remove the column \"confounds\" from the index and make the difference\nbetween the metrics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_cr_metrics = df_cr_metrics.reset_index().set_index([\"fold\", \"metric\"])\ndf_ncr_metrics = df_ncr_metrics.reset_index().set_index([\"fold\", \"metric\"])\n\ndf_diff_metrics = df_ncr_metrics[\"value\"] - df_cr_metrics[\"value\"]\ndf_diff_metrics = df_diff_metrics.reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can finally plot the difference, setting the whiskers of the box plot\nat 2.5 and 97.5 to see the 95% CI.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sns.boxplot(\n    x=\"metric\", y=\"value\", data=df_diff_metrics.reset_index(), whis=[2.5, 97.5]\n)\nplt.axhline(0, color=\"k\", ls=\":\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that while it seems that the accuracy and ROC AUC scores are\nhigher when confounds are not removed. We can not really claim (using this\ntest), that the models are different in terms of these metrics.\n\nMaybe the percentiles will be more accuracy with the proper amount of\nbootstrap iterations?\n\nBut the main point of confound removal is for interpretability. Let's see\nif there is a change in the feature importances.\n\nFirst, we need to collect the feature importances for each model, for each\nfold.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ncr_fi = []\nfor i_fold, estimator in enumerate(scores_ncr[\"estimator\"]):\n    this_importances = pd.DataFrame(\n        {\n            \"feature\": [x.replace(\"_\", \" \") for x in X],\n            \"importance\": estimator[\"rf\"].feature_importances_,\n            \"confounds\": \"Not Removed\",\n            \"fold\": i_fold,\n        }\n    )\n    ncr_fi.append(this_importances)\nncr_fi = pd.concat(ncr_fi)\n\ncr_fi = []\nfor i_fold, estimator in enumerate(scores_cr[\"estimator\"]):\n    this_importances = pd.DataFrame(\n        {\n            \"feature\": [x.replace(\"_\", \" \") for x in X],\n            \"importance\": estimator[\"rf\"].model.feature_importances_,\n            \"confounds\": \"Removed\",\n            \"fold\": i_fold,\n        }\n    )\n    cr_fi.append(this_importances)\ncr_fi = pd.concat(cr_fi)\n\nfeature_importance = pd.concat([cr_fi, ncr_fi])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot the importances.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sns.catplot(\n    x=\"feature\",\n    y=\"importance\",\n    hue=\"confounds\",\n    dodge=True,\n    data=feature_importance,\n    kind=\"swarm\",\n    s=3,\n)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And check the differences in importances. We can now see that there is\na difference in importances.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diff_fi = (\n    cr_fi.set_index([\"feature\", \"fold\"])[\"importance\"]\n    - ncr_fi.set_index([\"feature\", \"fold\"])[\"importance\"]\n)\nsns.boxplot(\n    x=\"importance\", y=\"feature\", data=diff_fi.reset_index(), whis=[2.5, 97.5]\n)\nplt.axvline(0, color=\"k\", ls=\":\")\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}